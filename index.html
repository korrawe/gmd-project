<!DOCTYPE html>
<html>
<head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-N9SB0YPDRC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-N9SB0YPDRC');
  </script>

  <meta charset="utf-8">
  <meta name="description"
        content="GMD: Guided Motion Diffusion Models">
  <meta name="keywords" content="GMD, guided motion diffusion, human motion, motion synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GMD: Guided Motion Diffusion Models</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="shortcut icon" href="favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GMD: Controllable Human Motion Synthesis via Guided Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vlg.inf.ethz.ch/team/Korrawe-Karunratanakul.html">Korrawe Karunratanakul</a><sup>1</sup>,</span> 
            <span class="author-block">
              <a href="https://konpat.me/">Konpat Preechakul</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.supasorn.com/">Supasorn Suwajanakorn</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Department of Computer Science, ETH Zurich </span> <br>
            <span class="author-block"><sup>2</sup>VISTEC, Thailand</span>
            <!-- <span class="author-block"><sup>1</sup>ETH Zurich</span> -->
          </div>

          <h3 class="is-5 conference">ICCV 2023</h3>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2212."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2305.12577"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=giw0pLIKdsA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/korrawe/guided-motion-diffusion"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg"
                 class="column is-centered has-text-centered"
                 alt="Teaser image."/>

      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        <b> Guided Motion Diffusion (GMD) </b> model can synthesize realistic human motion according to a text prompt, a reference trajectory, and key locations, as well as avoiding hitting your toe on giant X-mark circles that someone dropped on the floor. No need to retrain diffusion models for each of these tasks!
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Denoising diffusion models have shown great promise in human motion synthesis conditioned on natural language descriptions. However, it remains a challenge to integrate spatial constraints, such as pre-defined motion trajectories and obstacles, which is essential for bridging the gap between isolated human motion and its surrounding environment. 
          </p>
          <p>
            To address this issue, we propose Guided Motion Diffusion (GMD), a method that incorporates spatial constraints into the motion generation process. 
            Specifically, we propose an effective feature projection scheme that largely enhances the coherency between spatial information and local poses. Together with a new imputation formulation, the generated motion can reliably conform to spatial constraints such as global motion trajectories. 
          </p>
          <p>
            Furthermore, given sparse spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance approach that utilizes the denoiser of diffusion models to turn a sparse signal into denser signals, effectively guiding the generation motion to the given constraints. 
          </p>
          <p>
            The extensive experiments justify the development of GMD, which achieves a significant improvement over state-of-the-art methods in text-based motion generation while being able to control the synthesized motions with spatial constraints.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Result Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/giw0pLIKdsA;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Summary -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Summary</h2>
        <div class="content has-text-justified">
          <h4>Problems</h4>
          <ul>
            <li>To make the synthesized motions useful, we need a way to ground them in the 3D world. However, existing works synthesize motion based on text alone without control over where the character is heading to</li>
            <li>There are two main issues when trying to condition the motion diffusion model on a given spatial objective that makes the model likely to ignore the conditioning: the <b>sparseness of location in the motion representation</b> and <b>sparse guidance signals</b></li>
            <li>The sparseness of the guidance signals means the guidance will likely be ignored during the backward step. <b>Why?</b> Because it is easier to change a few values to make them coherent with the rest than the other way around </li>
            <li><b>Sparseness in the representation:</b> In each frame, the global location only consists of 4 values out of 263 values in the motion representation. The model is likely to change the global location to match the local pose than vice-versa due to the small importance of the global location</li>
            <li><b>Sparse guidance signals: </b>Additionally, the spatial guidance is also sparse as they are only defined on a few keyframes</li>
          </ul>
          <figure  class="content has-text-centered">
            <img src="./static/images/guidance_problem.jpg"
                   class="column is-7 is-offset-1 has-text-centered"
                   alt="Guidance problem."
                   />
          </figure >
          <h4>Our Solutions</h4>
          <ul>
            <li><b>Emphasis projection</b> allows us to manipulate feature importance <it>before training</it>. We give more emphasis to the global locations which are underrepresented in the motion vectors, making it harder for the model to ignore them.</li>
            <li><b>Dense gradient propagation</b> exploits the nature of denoising functions to propagate gradient at a specific frame to its neighboring frames, making the gradient from guidance dense in the frame dimension</li>
          </ul>
          </p>
        </div>
        
      </div>
    </div>

    <!-- Pipeline -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Our Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            In GMD, we tackle the problem of spatially conditioned motion generation using a two-staged pipeline (a) The optional first stage generates a trajectory given spatial conditioning. Then, the second stage synthesizes motions accroding to the trajectory. Our main contributions are (b) <b>Emphasis projection</b>, for better trajectory-motion coherence, and (c) <b>Dense signal propagation</b>, for a more controllable generation even under sparse guidance signal.
          </p>
        </div>
        <figure  class="content has-text-centered">
          <img src="./static/images/pipeline.jpg"
                 class="column is-8 is-offset-2 has-text-centered"
                 alt="Method pipeline."
                 />
        </figure >
        
        <!-- Emphasis projection -->
        <h3 class="title is-4 has-text-left">Emphasis projection</h3>
        <div class="content has-text-justified">
          <p>
            The most straightforward way of forcing the model to put more importance on the trajectory is to simply scale up its loss weight compared to other parts during training. Nevertheless, we found that loss manipulation is ineffective in this setting and also makes the training less stable.
            To achieve the same effect, we can scale up the value of the trajectory in the motion representation (multiply the values by c). But, doing so means the variance of each value in the motion vector will not be the same, which most diffusion models assume to be the case.
            The emphasis projection is then the next most straightforward solution for adjusting trajectory importance while taking these problems into account. It can be described as follows:
            <ol>
            <li>Scale up the trajectory values by a factor c.</li>
            <li>Randomly project the motion vector into a new vector using a fixed random matrix. Do the same for every frame.</li>
            <li>Normalize the new motion vector such that it has a unit variance again.</li>
            </ol>
            The final vectors are used as motion representation instead of the original representation.</li>
          </p>
        </div>
        <!--/ Emphasis projection -->

        <!-- Dense signal propagation -->
        <h3 class="title is-4 has-text-left">Dense signal propagation</h3>
        <div class="content has-text-justified">
          <p>
            Our goal is to convert the sparse guidance signals that are defined on some keyframes to denser signals during denoising.
            As the spatial guidance can only be defined on the clean motion (X<sub>0</sub>) but we need gradients w.r.t. the noisy motion (X<sub>t</sub>) to guide the motion at the denoising step t, we observe that
            <ol>
              <li>We need a function that can propagate gradients that are defined on some frames of X<sub>0</sub> to their surrounding frames but with respect to X<sub>t</sub> (not X<sub>0</sub> !)</li>
              <li>By definition, any denoising function (f: x<sub>noisy</sub> → x<sub>clean</sub>) that produces a clean motion from noisy motion has this dense gradient propagation property because it needs to look at the context to refine any given frame (let x<sub>noisy</sub>=x<sub>t</sub> and x<sub>clean</sub>=x<sub>0</sub>).</li>
              <li>This coincides with the definition of the diffusion model x<sub>0,θ</sub> which predicts the clean motion X<sub>0</sub> from the noisy motion X<sub>t</sub>. </li>
            </ol>
            As such, we can use the diffusion model itself to efficiently compute the gradients. In practice, this translates to using autodiff to compute the guidance gradients w.r.t. the input of the diffusion model.
          </p>
        </div>
        <!--/ Dense signal propagation -->
      </div>
    </div>
    
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Hand Avatar</h2>

        <h3 class="title is-4">Robustness</h3>
        <div class="content has-text-justified">
          <p>
            Given a short video, HARP can create an avatar from different capturing scenarios.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/other_dataset.jpg"
                 class="column is-centered has-text-centered"
                 alt="results in different dataset."/>
        </div>
        <br/>

        <h3 class="title is-4">Out-of-distribution appearance</h3>
        <div class="content has-text-justified">
          <p>
            HARP works without modification for out-of-distribution appearance that cannot be captured by parametric models.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/tattoo_single.jpg"
                 class="column is-centered has-text-centered"
                 alt="Out-of-distribution result."/>
        </div>


      </div>
    </div> -->



  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{karunratanakul2023gmd,
  author    = {Karunratanakul, Korrawe and Preechakul, Konpat and Suwajanakorn, Supasorn and Tang, Siyu},
  title     = {GMD: Controllable Human Motion Synthesis via Guided Diffusion Models},
  journal   = {arXiv preprint arXiv:2305.12577},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We use the website template provided by <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
